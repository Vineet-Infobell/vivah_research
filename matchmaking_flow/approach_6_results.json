{
  "approach": "Approach 6: Age Range + Semantic + Gaussian Age",
  "configuration": {
    "filters": [
      "gender",
      "religion",
      "age_range"
    ],
    "semantic_weight": 0.8746,
    "age_weight": 0.1254,
    "older_base": 0.8938,
    "younger_base": 0.9091
  },
  "summary": {
    "average_latency_ms": 1147.83,
    "average_ndcg": 0.682,
    "average_precision": 64.0,
    "average_recall": 38.2,
    "total_scenarios": 5
  },
  "scenarios": [
    {
      "scenario_id": 1,
      "latency_ms": 2318.88,
      "embedding_time_ms": 2304.7,
      "db_time_ms": 9.03,
      "ndcg": 0.702,
      "precision": 60.0,
      "recall": 40.0,
      "predicted_ids": [
        901,
        794,
        903,
        446,
        905,
        906,
        918,
        732,
        660,
        917
      ],
      "ground_truth_ids": [
        732,
        904,
        908,
        901,
        906,
        916,
        794,
        905,
        907,
        902,
        903,
        912,
        909,
        913,
        245
      ]
    },
    {
      "scenario_id": 2,
      "latency_ms": 551.54,
      "embedding_time_ms": 497.59,
      "db_time_ms": 49.47,
      "ndcg": 0.672,
      "precision": 60.0,
      "recall": 30.0,
      "predicted_ids": [
        256,
        853,
        930,
        922,
        862,
        246,
        705,
        100,
        925,
        161
      ],
      "ground_truth_ids": [
        705,
        853,
        928,
        246,
        590,
        677,
        925,
        927,
        935,
        936,
        256,
        922,
        923,
        924,
        134,
        178,
        188,
        407,
        492,
        564
      ]
    },
    {
      "scenario_id": 3,
      "latency_ms": 1137.97,
      "embedding_time_ms": 1077.17,
      "db_time_ms": 59.15,
      "ndcg": 0.453,
      "precision": 60.0,
      "recall": 31.6,
      "predicted_ids": [
        287,
        951,
        947,
        799,
        314,
        948,
        684,
        530,
        584,
        944
      ],
      "ground_truth_ids": [
        943,
        944,
        945,
        947,
        955,
        948,
        942,
        946,
        287,
        951,
        956,
        941,
        864,
        949,
        952,
        457,
        653,
        799,
        950
      ]
    },
    {
      "scenario_id": 4,
      "latency_ms": 597.78,
      "embedding_time_ms": 547.05,
      "db_time_ms": 49.72,
      "ndcg": 0.649,
      "precision": 50.0,
      "recall": 33.3,
      "predicted_ids": [
        962,
        961,
        966,
        965,
        975,
        972,
        979,
        980,
        378,
        3
      ],
      "ground_truth_ids": [
        101,
        963,
        964,
        967,
        968,
        973,
        961,
        962,
        965,
        966,
        975,
        974,
        976,
        970,
        969
      ]
    },
    {
      "scenario_id": 5,
      "latency_ms": 1133.0,
      "embedding_time_ms": 1077.74,
      "db_time_ms": 52.7,
      "ndcg": 0.934,
      "precision": 90.0,
      "recall": 56.2,
      "predicted_ids": [
        985,
        986,
        981,
        993,
        982,
        984,
        988,
        989,
        998,
        987
      ],
      "ground_truth_ids": [
        987,
        990,
        994,
        995,
        981,
        982,
        983,
        986,
        985,
        989,
        993,
        984,
        988,
        996,
        991,
        992
      ]
    }
  ]
}