{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Google Embedding Models Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.oauth2 import service_account\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded .env from: f:\\Vivahai\\Vivahai_backend_v2\\research\\..\\vivah_api\\.env\n",
      "\n",
      "ðŸ“‹ Configuration:\n",
      "   Project: triple-name-473801-t7\n",
      "   Location: us-central1\n",
      "   Credentials: f:\\Vivahai\\Vivahai_backend_v2\\research\\..\\vivah_api\\service-account.json\n",
      "   Exists: True\n"
     ]
    }
   ],
   "source": [
    "# Load Credentials\n",
    "\n",
    "# Load environment variables\n",
    "import os  # <-- Add this line\n",
    "env_path = Path('../vivah_api/.env')\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"âœ… Loaded .env from: {env_path.absolute()}\")\n",
    "else:\n",
    "    load_dotenv()\n",
    "    print(\"âš ï¸  Using default .env\")\n",
    "\n",
    "# Get credentials path\n",
    "CREDENTIALS_PATH = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "if CREDENTIALS_PATH and not os.path.isabs(CREDENTIALS_PATH):\n",
    "    CREDENTIALS_PATH = str((Path('../vivah_api') / CREDENTIALS_PATH).absolute())\n",
    "\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\", \"triple-name-473801-t7\")\n",
    "LOCATION = os.getenv(\"GCP_LOCATION\", \"us-central1\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Configuration:\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Location: {LOCATION}\")\n",
    "print(f\"   Credentials: {CREDENTIALS_PATH}\")\n",
    "print(f\"   Exists: {os.path.exists(CREDENTIALS_PATH) if CREDENTIALS_PATH else False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Google GenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Google GenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google GenAI Client\n",
    "\n",
    "# Create credentials\n",
    "SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    CREDENTIALS_PATH, scopes=SCOPES\n",
    ")\n",
    "\n",
    "# Initialize GenAI client\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    credentials=credentials,\n",
    ")\n",
    "\n",
    "print(\"âœ… Google GenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Test Configuration:\n",
      "   Models: 4\n",
      "   Dimensions: [768, 1152, 1536, 3072]\n",
      "   Total tests: 16\n",
      "   Test text: 'Looking for a life partner who is educated, kind and family oriented.'\n"
     ]
    }
   ],
   "source": [
    "# Test Configuration\n",
    "\n",
    "# Models to test\n",
    "MODELS = [\n",
    "    \"text-embedding-004\",\n",
    "    \"gemini-embedding-001\",\n",
    "    \"text-embedding-005\",\n",
    "    \"text-multilingual-embedding-002\"\n",
    "]\n",
    "\n",
    "# Dimensions to test\n",
    "DIMENSIONS = [768, 1152, 1536, 3072]\n",
    "\n",
    "# Test text\n",
    "TEST_TEXT = \"Looking for a life partner who is educated, kind and family oriented.\"\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Configuration:\")\n",
    "print(f\"   Models: {len(MODELS)}\")\n",
    "print(f\"   Dimensions: {DIMENSIONS}\")\n",
    "print(f\"   Total tests: {len(MODELS) * len(DIMENSIONS)}\")\n",
    "print(f\"   Test text: '{TEST_TEXT}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Benchmark function defined\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Function\n",
    "\n",
    "def test_embedding(\n",
    "    model_name: str, \n",
    "    dimension: int, \n",
    "    text: str = TEST_TEXT\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Test embedding generation for a specific model and dimension.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with test results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'model_name': model_name,\n",
    "        'requested_dim': dimension,\n",
    "        'actual_dim': None,\n",
    "        'status': 'FAILED',\n",
    "        'latency_ms': None,\n",
    "        'remarks': ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate embedding\n",
    "        response = client.models.embed_content(\n",
    "            model=model_name,\n",
    "            contents=text,\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\",\n",
    "                output_dimensionality=dimension\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Calculate latency\n",
    "        latency = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        # Get embedding vector\n",
    "        embedding = response.embeddings[0].values\n",
    "        actual_dim = len(embedding)\n",
    "        \n",
    "        # Update result\n",
    "        result['actual_dim'] = actual_dim\n",
    "        result['latency_ms'] = round(latency, 2)\n",
    "        result['status'] = 'SUCCESS'\n",
    "        \n",
    "        # Check if dimension matches\n",
    "        if actual_dim == dimension:\n",
    "            result['remarks'] = 'âœ… Dimension matches'\n",
    "        else:\n",
    "            result['remarks'] = f'âš ï¸ Dimension mismatch: requested {dimension}, got {actual_dim}'\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        result['remarks'] = f'âŒ Error: {error_msg[:100]}'\n",
    "        \n",
    "        # Categorize error\n",
    "        if 'not found' in error_msg.lower():\n",
    "            result['remarks'] = 'âŒ Model not found'\n",
    "        elif 'dimension' in error_msg.lower():\n",
    "            result['remarks'] = 'âŒ Unsupported dimension'\n",
    "        elif 'quota' in error_msg.lower():\n",
    "            result['remarks'] = 'âŒ Quota exceeded'\n",
    "        elif 'permission' in error_msg.lower():\n",
    "            result['remarks'] = 'âŒ Permission denied'\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… Benchmark function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting benchmark tests...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¦ Testing model: text-embedding-004\n",
      "--------------------------------------------------------------------------------\n",
      "   [1/16] Dimension: 768... âœ… 768D in 3067.17ms\n",
      "   [2/16] Dimension: 1152... âŒ âŒ Unsupported dimension\n",
      "   [3/16] Dimension: 1536... âŒ âŒ Unsupported dimension\n",
      "   [4/16] Dimension: 3072... âŒ âŒ Unsupported dimension\n",
      "\n",
      "ðŸ“¦ Testing model: gemini-embedding-001\n",
      "--------------------------------------------------------------------------------\n",
      "   [5/16] Dimension: 768... âœ… 768D in 726.98ms\n",
      "   [6/16] Dimension: 1152... âœ… 1152D in 496.51ms\n",
      "   [7/16] Dimension: 1536... âœ… 1536D in 740.49ms\n",
      "   [8/16] Dimension: 3072... âœ… 3072D in 710.89ms\n",
      "\n",
      "ðŸ“¦ Testing model: text-embedding-005\n",
      "--------------------------------------------------------------------------------\n",
      "   [9/16] Dimension: 768... âœ… 768D in 416.58ms\n",
      "   [10/16] Dimension: 1152... âŒ âŒ Unsupported dimension\n",
      "   [11/16] Dimension: 1536... âŒ âŒ Unsupported dimension\n",
      "   [12/16] Dimension: 3072... âŒ âŒ Unsupported dimension\n",
      "\n",
      "ðŸ“¦ Testing model: text-multilingual-embedding-002\n",
      "--------------------------------------------------------------------------------\n",
      "   [13/16] Dimension: 768... âœ… 768D in 475.06ms\n",
      "   [14/16] Dimension: 1152... âŒ âŒ Unsupported dimension\n",
      "   [15/16] Dimension: 1536... âŒ âŒ Unsupported dimension\n",
      "   [16/16] Dimension: 3072... âŒ âŒ Unsupported dimension\n",
      "\n",
      "================================================================================\n",
      "âœ… Benchmark complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Benchmark Tests\n",
    "\n",
    "print(\"\\nðŸš€ Starting benchmark tests...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "total_tests = len(MODELS) * len(DIMENSIONS)\n",
    "current_test = 0\n",
    "\n",
    "for model in MODELS:\n",
    "    print(f\"\\nðŸ“¦ Testing model: {model}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for dim in DIMENSIONS:\n",
    "        current_test += 1\n",
    "        print(f\"   [{current_test}/{total_tests}] Dimension: {dim}...\", end=\" \")\n",
    "        \n",
    "        # Run test\n",
    "        result = test_embedding(model, dim)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print result\n",
    "        if result['status'] == 'SUCCESS':\n",
    "            print(f\"âœ… {result['actual_dim']}D in {result['latency_ms']}ms\")\n",
    "        else:\n",
    "            print(f\"âŒ {result['remarks']}\")\n",
    "        \n",
    "        # Rate limiting (avoid quota issues)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Benchmark complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š COMPLETE RESULTS TABLE\n",
      "================================================================================\n",
      "                         model_name  requested_dim  actual_dim   status  latency_ms                  remarks\n",
      "0                text-embedding-004            768       768.0  SUCCESS     3067.17      âœ… Dimension matches\n",
      "1                text-embedding-004           1152         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "2                text-embedding-004           1536         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "3                text-embedding-004           3072         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "4              gemini-embedding-001            768       768.0  SUCCESS      726.98      âœ… Dimension matches\n",
      "5              gemini-embedding-001           1152      1152.0  SUCCESS      496.51      âœ… Dimension matches\n",
      "6              gemini-embedding-001           1536      1536.0  SUCCESS      740.49      âœ… Dimension matches\n",
      "7              gemini-embedding-001           3072      3072.0  SUCCESS      710.89      âœ… Dimension matches\n",
      "8                text-embedding-005            768       768.0  SUCCESS      416.58      âœ… Dimension matches\n",
      "9                text-embedding-005           1152         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "10               text-embedding-005           1536         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "11               text-embedding-005           3072         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "12  text-multilingual-embedding-002            768       768.0  SUCCESS      475.06      âœ… Dimension matches\n",
      "13  text-multilingual-embedding-002           1152         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "14  text-multilingual-embedding-002           1536         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "15  text-multilingual-embedding-002           3072         NaN   FAILED         NaN  âŒ Unsupported dimension\n",
      "\n",
      "ðŸ’¾ Results saved to: embedding_benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Results Table\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display full table\n",
    "print(\"ðŸ“Š COMPLETE RESULTS TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = 'embedding_benchmark_results.csv'\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Success Rate by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ðŸ“ˆ SUCCESS RATE BY MODEL\n",
      "================================================================================\n",
      "\n",
      "text-embedding-004:\n",
      "   Success: 1/4 (25.0%)\n",
      "   Supported dimensions: [768.0]\n",
      "   Latency: avg=3067.17ms, min=3067.17ms, max=3067.17ms\n",
      "\n",
      "gemini-embedding-001:\n",
      "   Success: 4/4 (100.0%)\n",
      "   Supported dimensions: [768.0, 1152.0, 1536.0, 3072.0]\n",
      "   Latency: avg=668.72ms, min=496.51ms, max=740.49ms\n",
      "\n",
      "text-embedding-005:\n",
      "   Success: 1/4 (25.0%)\n",
      "   Supported dimensions: [768.0]\n",
      "   Latency: avg=416.58ms, min=416.58ms, max=416.58ms\n",
      "\n",
      "text-multilingual-embedding-002:\n",
      "   Success: 1/4 (25.0%)\n",
      "   Supported dimensions: [768.0]\n",
      "   Latency: avg=475.06ms, min=475.06ms, max=475.06ms\n"
     ]
    }
   ],
   "source": [
    "# Analysis: Success Rate by Model\n",
    "\n",
    "print(\"\\n\\nðŸ“ˆ SUCCESS RATE BY MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model in MODELS:\n",
    "    model_results = df_results[df_results['model_name'] == model]\n",
    "    success_count = len(model_results[model_results['status'] == 'SUCCESS'])\n",
    "    total_count = len(model_results)\n",
    "    success_rate = (success_count / total_count) * 100\n",
    "    \n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"   Success: {success_count}/{total_count} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # Show supported dimensions\n",
    "    successful = model_results[model_results['status'] == 'SUCCESS']\n",
    "    if len(successful) > 0:\n",
    "        supported_dims = successful['actual_dim'].tolist()\n",
    "        print(f\"   Supported dimensions: {supported_dims}\")\n",
    "        \n",
    "        # Show latency stats\n",
    "        avg_latency = successful['latency_ms'].mean()\n",
    "        min_latency = successful['latency_ms'].min()\n",
    "        max_latency = successful['latency_ms'].max()\n",
    "        print(f\"   Latency: avg={avg_latency:.2f}ms, min={min_latency:.2f}ms, max={max_latency:.2f}ms\")\n",
    "    else:\n",
    "        print(f\"   âŒ No successful tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Success Rate by Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ðŸ“Š SUCCESS RATE BY DIMENSION\n",
      "================================================================================\n",
      "\n",
      "768 dimensions:\n",
      "   Success: 4/4 (100.0%)\n",
      "   Supported by: text-embedding-004, gemini-embedding-001, text-embedding-005, text-multilingual-embedding-002\n",
      "\n",
      "1152 dimensions:\n",
      "   Success: 1/4 (25.0%)\n",
      "   Supported by: gemini-embedding-001\n",
      "\n",
      "1536 dimensions:\n",
      "   Success: 1/4 (25.0%)\n",
      "   Supported by: gemini-embedding-001\n",
      "\n",
      "3072 dimensions:\n",
      "   Success: 1/4 (25.0%)\n",
      "   Supported by: gemini-embedding-001\n"
     ]
    }
   ],
   "source": [
    "# Analysis: Success Rate by Dimension\n",
    "\n",
    "print(\"\\n\\nðŸ“Š SUCCESS RATE BY DIMENSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dim in DIMENSIONS:\n",
    "    dim_results = df_results[df_results['requested_dim'] == dim]\n",
    "    success_count = len(dim_results[dim_results['status'] == 'SUCCESS'])\n",
    "    total_count = len(dim_results)\n",
    "    success_rate = (success_count / total_count) * 100\n",
    "    \n",
    "    print(f\"\\n{dim} dimensions:\")\n",
    "    print(f\"   Success: {success_count}/{total_count} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # Show which models support this dimension\n",
    "    successful = dim_results[dim_results['status'] == 'SUCCESS']\n",
    "    if len(successful) > 0:\n",
    "        supported_models = successful['model_name'].tolist()\n",
    "        print(f\"   Supported by: {', '.join(supported_models)}\")\n",
    "    else:\n",
    "        print(f\"   âŒ Not supported by any model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "âš¡ LATENCY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Average Latency (ms) by Model and Dimension:\n",
      "requested_dim                       768     1152    1536    3072\n",
      "model_name                                                      \n",
      "gemini-embedding-001              726.98  496.51  740.49  710.89\n",
      "text-embedding-004               3067.17     NaN     NaN     NaN\n",
      "text-embedding-005                416.58     NaN     NaN     NaN\n",
      "text-multilingual-embedding-002   475.06     NaN     NaN     NaN\n",
      "\n",
      "ðŸ† Fastest model overall: text-embedding-005 (416.58ms avg)\n",
      "ðŸ† Fastest dimension: 1152D (496.51ms avg)\n"
     ]
    }
   ],
   "source": [
    "# Analysis: Latency Comparison\n",
    "\n",
    "print(\"\\n\\nâš¡ LATENCY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter successful tests only\n",
    "successful_tests = df_results[df_results['status'] == 'SUCCESS'].copy()\n",
    "\n",
    "if len(successful_tests) > 0:\n",
    "    # Pivot table for better visualization\n",
    "    latency_pivot = successful_tests.pivot_table(\n",
    "        values='latency_ms',\n",
    "        index='model_name',\n",
    "        columns='requested_dim',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAverage Latency (ms) by Model and Dimension:\")\n",
    "    print(latency_pivot.to_string())\n",
    "    \n",
    "    # Find fastest model\n",
    "    avg_latency_by_model = successful_tests.groupby('model_name')['latency_ms'].mean().sort_values()\n",
    "    print(f\"\\nðŸ† Fastest model overall: {avg_latency_by_model.index[0]} ({avg_latency_by_model.iloc[0]:.2f}ms avg)\")\n",
    "    \n",
    "    # Find fastest dimension\n",
    "    avg_latency_by_dim = successful_tests.groupby('requested_dim')['latency_ms'].mean().sort_values()\n",
    "    print(f\"ðŸ† Fastest dimension: {avg_latency_by_dim.index[0]}D ({avg_latency_by_dim.iloc[0]:.2f}ms avg)\")\n",
    "else:\n",
    "    print(\"âŒ No successful tests to analyze latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Supported Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ðŸ“ MAXIMUM SUPPORTED DIMENSIONS\n",
      "================================================================================\n",
      "\n",
      "text-embedding-004:\n",
      "   Max dimension: 768.0\n",
      "   All supported: [np.float64(768.0)]\n",
      "\n",
      "gemini-embedding-001:\n",
      "   Max dimension: 3072.0\n",
      "   All supported: [np.float64(768.0), np.float64(1152.0), np.float64(1536.0), np.float64(3072.0)]\n",
      "\n",
      "text-embedding-005:\n",
      "   Max dimension: 768.0\n",
      "   All supported: [np.float64(768.0)]\n",
      "\n",
      "text-multilingual-embedding-002:\n",
      "   Max dimension: 768.0\n",
      "   All supported: [np.float64(768.0)]\n"
     ]
    }
   ],
   "source": [
    "# Maximum Supported Dimensions\n",
    "\n",
    "print(\"\\n\\nðŸ“ MAXIMUM SUPPORTED DIMENSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model in MODELS:\n",
    "    model_results = df_results[\n",
    "        (df_results['model_name'] == model) & \n",
    "        (df_results['status'] == 'SUCCESS')\n",
    "    ]\n",
    "    \n",
    "    if len(model_results) > 0:\n",
    "        max_dim = model_results['actual_dim'].max()\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"   Max dimension: {max_dim}\")\n",
    "        print(f\"   All supported: {sorted(model_results['actual_dim'].unique())}\")\n",
    "    else:\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"   âŒ No successful tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ CONCLUSIONS & RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "ðŸ“‹ FINAL CONCLUSIONS & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ MODELS WITH CONFIGURABLE DIMENSIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… text-embedding-004\n",
      "   Supports: [np.float64(768.0)]\n",
      "âœ… gemini-embedding-001\n",
      "   Supports: [np.float64(768.0), np.float64(1152.0), np.float64(1536.0), np.float64(3072.0)]\n",
      "âœ… text-embedding-005\n",
      "   Supports: [np.float64(768.0)]\n",
      "âœ… text-multilingual-embedding-002\n",
      "   Supports: [np.float64(768.0)]\n",
      "\n",
      "2ï¸âƒ£ MAXIMUM SUPPORTED DIMENSIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "Highest dimension tested successfully: 3072.0\n",
      "Models supporting 3072.0D: gemini-embedding-001\n",
      "\n",
      "3ï¸âƒ£ PRODUCTION RECOMMENDATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸŽ¯ Most Reliable Dimension: 768\n",
      "   Success rate: 100.0%\n",
      "   Supported by 4 models\n",
      "\n",
      "ðŸ† Recommended Model: text-embedding-005\n",
      "   Average latency: 416.58ms\n",
      "   Successful tests: 1\n",
      "\n",
      "ðŸ’¡ USE CASE RECOMMENDATIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âš¡ For Speed (lowest latency):\n",
      "   Model: text-embedding-005\n",
      "   Dimension: 768.0\n",
      "   Latency: 416.58ms\n",
      "\n",
      "ðŸŽ¯ For Accuracy (highest dimension):\n",
      "   Model: gemini-embedding-001\n",
      "   Dimension: 3072.0\n",
      "   Latency: 710.89ms\n",
      "\n",
      "âš–ï¸ For Balance (768D - speed + accuracy):\n",
      "   Model: text-embedding-005\n",
      "   Dimension: 768.0\n",
      "   Latency: 416.58ms\n",
      "\n",
      "================================================================================\n",
      "âœ… BENCHMARK ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CONCLUSIONS & RECOMMENDATIONS\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ FINAL CONCLUSIONS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Models with dimension control\n",
    "print(\"\\n1ï¸âƒ£ MODELS WITH CONFIGURABLE DIMENSIONS:\")\n",
    "print(\"-\" * 80)\n",
    "for model in MODELS:\n",
    "    model_results = df_results[\n",
    "        (df_results['model_name'] == model) & \n",
    "        (df_results['status'] == 'SUCCESS')\n",
    "    ]\n",
    "    if len(model_results) > 0:\n",
    "        dims = sorted(model_results['actual_dim'].unique())\n",
    "        print(f\"âœ… {model}\")\n",
    "        print(f\"   Supports: {dims}\")\n",
    "    else:\n",
    "        print(f\"âŒ {model}\")\n",
    "        print(f\"   No dimension control or model unavailable\")\n",
    "\n",
    "# 2. Maximum dimensions\n",
    "print(\"\\n2ï¸âƒ£ MAXIMUM SUPPORTED DIMENSIONS:\")\n",
    "print(\"-\" * 80)\n",
    "successful_models = df_results[df_results['status'] == 'SUCCESS']['model_name'].unique()\n",
    "if len(successful_models) > 0:\n",
    "    max_overall = df_results[df_results['status'] == 'SUCCESS']['actual_dim'].max()\n",
    "    print(f\"Highest dimension tested successfully: {max_overall}\")\n",
    "    \n",
    "    models_with_max = df_results[\n",
    "        (df_results['status'] == 'SUCCESS') & \n",
    "        (df_results['actual_dim'] == max_overall)\n",
    "    ]['model_name'].unique()\n",
    "    print(f\"Models supporting {max_overall}D: {', '.join(models_with_max)}\")\n",
    "else:\n",
    "    print(\"âŒ No successful tests\")\n",
    "\n",
    "# 3. Production recommendations\n",
    "print(\"\\n3ï¸âƒ£ PRODUCTION RECOMMENDATIONS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(successful_tests) > 0:\n",
    "    # Find most reliable dimension (highest success rate)\n",
    "    dim_success_rate = df_results.groupby('requested_dim').apply(\n",
    "        lambda x: (x['status'] == 'SUCCESS').sum() / len(x) * 100\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    best_dim = dim_success_rate.index[0]\n",
    "    best_dim_rate = dim_success_rate.iloc[0]\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Most Reliable Dimension: {best_dim}\")\n",
    "    print(f\"   Success rate: {best_dim_rate:.1f}%\")\n",
    "    print(f\"   Supported by {len(df_results[(df_results['requested_dim'] == best_dim) & (df_results['status'] == 'SUCCESS')])} models\")\n",
    "    \n",
    "    # Find best model (highest success rate + lowest latency)\n",
    "    model_stats = successful_tests.groupby('model_name').agg({\n",
    "        'latency_ms': 'mean',\n",
    "        'status': 'count'\n",
    "    }).rename(columns={'status': 'success_count'})\n",
    "    \n",
    "    if len(model_stats) > 0:\n",
    "        best_model = model_stats['latency_ms'].idxmin()\n",
    "        best_latency = model_stats.loc[best_model, 'latency_ms']\n",
    "        \n",
    "        print(f\"\\nðŸ† Recommended Model: {best_model}\")\n",
    "        print(f\"   Average latency: {best_latency:.2f}ms\")\n",
    "        print(f\"   Successful tests: {model_stats.loc[best_model, 'success_count']}\")\n",
    "    \n",
    "    # Specific use case recommendations\n",
    "    print(\"\\nðŸ’¡ USE CASE RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Speed priority\n",
    "    if len(successful_tests) > 0:\n",
    "        fastest = successful_tests.loc[successful_tests['latency_ms'].idxmin()]\n",
    "        print(f\"\\nâš¡ For Speed (lowest latency):\")\n",
    "        print(f\"   Model: {fastest['model_name']}\")\n",
    "        print(f\"   Dimension: {fastest['actual_dim']}\")\n",
    "        print(f\"   Latency: {fastest['latency_ms']:.2f}ms\")\n",
    "    \n",
    "    # Accuracy priority (highest dimension)\n",
    "    highest_dim_tests = successful_tests[successful_tests['actual_dim'] == successful_tests['actual_dim'].max()]\n",
    "    if len(highest_dim_tests) > 0:\n",
    "        best_high_dim = highest_dim_tests.loc[highest_dim_tests['latency_ms'].idxmin()]\n",
    "        print(f\"\\nðŸŽ¯ For Accuracy (highest dimension):\")\n",
    "        print(f\"   Model: {best_high_dim['model_name']}\")\n",
    "        print(f\"   Dimension: {best_high_dim['actual_dim']}\")\n",
    "        print(f\"   Latency: {best_high_dim['latency_ms']:.2f}ms\")\n",
    "    \n",
    "    # Balanced\n",
    "    mid_dim_tests = successful_tests[successful_tests['actual_dim'] == 768]\n",
    "    if len(mid_dim_tests) > 0:\n",
    "        best_balanced = mid_dim_tests.loc[mid_dim_tests['latency_ms'].idxmin()]\n",
    "        print(f\"\\nâš–ï¸ For Balance (768D - speed + accuracy):\")\n",
    "        print(f\"   Model: {best_balanced['model_name']}\")\n",
    "        print(f\"   Dimension: {best_balanced['actual_dim']}\")\n",
    "        print(f\"   Latency: {best_balanced['latency_ms']:.2f}ms\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No successful tests to provide recommendations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… BENCHMARK ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ðŸ“Š SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total tests run: 16\n",
      "Successful: 7 (43.8%)\n",
      "Failed: 9 (56.2%)\n",
      "\n",
      "Latency statistics (successful tests):\n",
      "   Mean: 947.67ms\n",
      "   Median: 710.89ms\n",
      "   Min: 416.58ms\n",
      "   Max: 3067.17ms\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ‰ BENCHMARK COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary Statistics\n",
    "\n",
    "print(\"\\n\\nðŸ“Š SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_tests = len(df_results)\n",
    "successful_tests_count = len(df_results[df_results['status'] == 'SUCCESS'])\n",
    "failed_tests_count = len(df_results[df_results['status'] == 'FAILED'])\n",
    "\n",
    "print(f\"\\nTotal tests run: {total_tests}\")\n",
    "print(f\"Successful: {successful_tests_count} ({successful_tests_count/total_tests*100:.1f}%)\")\n",
    "print(f\"Failed: {failed_tests_count} ({failed_tests_count/total_tests*100:.1f}%)\")\n",
    "\n",
    "if successful_tests_count > 0:\n",
    "    print(f\"\\nLatency statistics (successful tests):\")\n",
    "    print(f\"   Mean: {df_results[df_results['status'] == 'SUCCESS']['latency_ms'].mean():.2f}ms\")\n",
    "    print(f\"   Median: {df_results[df_results['status'] == 'SUCCESS']['latency_ms'].median():.2f}ms\")\n",
    "    print(f\"   Min: {df_results[df_results['status'] == 'SUCCESS']['latency_ms'].min():.2f}ms\")\n",
    "    print(f\"   Max: {df_results[df_results['status'] == 'SUCCESS']['latency_ms'].max():.2f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ BENCHMARK COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
